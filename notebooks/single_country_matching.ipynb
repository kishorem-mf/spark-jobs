{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name matching\n",
    "\n",
    "This notebook will run the name matching algorithm for an specific country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `.egg` file from C++/Cython module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from glob import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! bash compile_library.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "egg_file = glob(path.join('cython', 'dist', '*.egg'))[0]\n",
    "egg_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark cluster configuration**\n",
    "\n",
    "We will be broadcasting a possible big sparse matrix, then it could be necessary to allocate up to 4-5GB of memory to each executor, the extreme cases are US or GB, for other countries this memory can be less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('NameMatching_Notebook')\n",
    "conf.setExecutorEnv('PYTHON_EGG_CACHE', '/tmp')\n",
    "# supposing you have 4 nodes with 8 cores each\n",
    "# NOTE: HDInsight by default allocates a single core per executor\n",
    "# regardless of what we set in here. The truth is in Yarn UI not in Spark UI.\n",
    "conf.set('spark.dynamicAllocation.enabled', False)\n",
    "conf.set('spark.executor.instances', 4) # low so that memory sharing is high\n",
    "conf.set('spark.executor.cores', 14)\n",
    "conf.set('spark.executor.memory', '14g')\n",
    "conf.set('spark.driver.memory', '15g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get a spark context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**send Cython code**\n",
    "\n",
    "For doing the sparse matrix multiplication we are using c++ code wrapped using Cython so we need to send this file to every worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sparse_dot_topn.sparse_dot_topn as ct # this is the cython module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "\n",
    "from pyspark.ml.param.shared import HasInputCol\n",
    "from pyspark.ml.param.shared import HasOutputCol\n",
    "from pyspark.ml.param.shared import Param\n",
    "from pyspark.ml.param.shared import Params\n",
    "\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType, FloatType\n",
    "from pyspark.mllib.linalg import VectorUDT, Vector, Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.functions import array, struct\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.sparse import _sparsetools\n",
    "from scipy.sparse.sputils import get_index_dtype\n",
    "import math\n",
    "from itertools import tee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cython sparse multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk_dot_limit(A, B, ntop, threshold=0, start_row=0):\n",
    "    \"\"\"Sparse dot product of two matrices AxB\n",
    "    \n",
    "    It will return the upper triangular matrix.\n",
    "    \n",
    "    Args:\n",
    "        A (csr matrix): Left matrix with shape m x k\n",
    "        B (csr matrix): Right matrix with shape k x n\n",
    "        ntop (int): Get only n most similar\n",
    "        threshold (float): Return similarities above this number\n",
    "        start_row (int): Set start row index for matrix A\n",
    "    \"\"\"\n",
    "    B = B.tocsr()\n",
    "    \n",
    "    M = A.shape[0]\n",
    "    N = B.shape[1]\n",
    "\n",
    "    idx_dtype = np.int32\n",
    "\n",
    "    # maximum number of possible non-zero element in the upper triangular matrix\n",
    "    nnz_max = min(M * (2 * (N - start_row) - M - 1) / 2, M * ntop)\n",
    "\n",
    "    # the arrays will be returned by reference\n",
    "    rows = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    cols = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    data = np.empty(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    # number of found non-zero entries in the upper triangular matrix\n",
    "    # I'll use this value to slice the returning numpy array\n",
    "    nnz = ct.sparse_dot_topn(\n",
    "        M, N,\n",
    "        np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        threshold, rows, cols, data, start_row)\n",
    "    \n",
    "    return ((int(i), int(j), float(v)) for i, j, v in zip(rows[:nnz], cols[:nnz], data[:nnz]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_code = 'DK'\n",
    "fraction = 1. # use only this fraction of the data\n",
    "threshold = 0.9 # ignore similarities below threshold\n",
    "ntop = 1000 # max number of similarities to match per name\n",
    "n_gram = 2 # size of letter n-grams to form\n",
    "vocabulary_size = 15000 # number of ngrams to take\n",
    "min_appearances = 2 # keep n-grams that appear at least this number of times in the corpus\n",
    "input_path = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS.parquet'\n",
    "output_path = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS_MATCHED.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load operator data\n",
    "\n",
    "- Load complete data\n",
    "- Get only the one for the specified country\n",
    "- Drop `NA` in column `NAME_CLEANSED`\n",
    "- Create unique `id` column\n",
    "- Create `name` column to use for deduplication\n",
    "- Clean `name` column\n",
    "- Create `dummy_id` columns which represent the row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "operators = sqlContext.read.parquet(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"name_index\", LongType(), False),\n",
    "                     StructField(\"id\", StringType(), False),\n",
    "                     StructField(\"name\", StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_chars = \"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "regex = \"[{}]\".format(drop_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "country_opr = (\n",
    "    operators[operators['COUNTRY_CODE'] == country_code]\n",
    "    .na.drop(subset=['NAME_CLEANSED'])\n",
    "    .sample(False, fraction)\n",
    "    .withColumn('id', sf.concat('COUNTRY_CODE', sf.lit(\"~\"), 'SOURCE', sf.lit('~'), 'REF_OPERATOR_ID'))\n",
    "    .fillna('')\n",
    "#     .withColumn('name', sf.concat('NAME_CLEANSED', sf.lit(' '), sf.col('CITY_CLEANSED'), sf.lit(' '), sf.col('STREET_CLEANSED'), sf.lit(' '), sf.col('ZIP_CODE_CLEANSED')))\n",
    "    .withColumn('name', sf.concat('NAME_CLEANSED', sf.lit(' '), sf.col('CITY_CLEANSED')))\n",
    "    .withColumn('name', sf.regexp_replace('name', regex, ''))\n",
    "    .withColumn('name', sf.trim(sf.regexp_replace('name', '\\s+', ' ')))\n",
    "    .select('id', 'name')\n",
    "    .sort('id', ascending=True)\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[1], x[0][0], x[0][1]))\n",
    "    .toDF(schema)\n",
    ")\n",
    "country_opr.persist()\n",
    "\n",
    "print(\"Nr Rows:\", country_opr.count())\n",
    "country_opr.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation pipeline\n",
    "\n",
    "1. `RegexTokenizer` to separate each character into an array\n",
    "2. `NGram` to create n-grams from the letters\n",
    "3. `CountVectorizer` to count number of occurences of any n-gram in each name (this can be changed at some point to `HashingTF`)\n",
    "4. `IDF` perform inverse document frequency of the n-grams created\n",
    "5. `Normalizer` to L2 normalize the resulting tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each character into a vector\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"name\", outputCol=\"tokens\", pattern=\"\")\n",
    "# create vector of n-grams\n",
    "ngram_creator = NGram(n=n_gram, inputCol=\"tokens\", outputCol=\"n_grams\")\n",
    "# term frequency\n",
    "tf_counter = CountVectorizer(minTF=1, minDF=min_appearances, vocabSize=vocabulary_size, \n",
    "                             inputCol='n_grams', outputCol='term_frequency', binary=False)\n",
    "\n",
    "# tf_counter = HashingTF(numFeatures=64, \n",
    "#                        inputCol='n_grams', outputCol='term_frequency', binary=False)\n",
    "\n",
    "# inverse document frequency\n",
    "idf_counter = IDF(inputCol=\"term_frequency\", outputCol=\"tfidf_vector\")\n",
    "# L2 normalization of vector\n",
    "l2_normalizer = Normalizer(inputCol=\"tfidf_vector\", outputCol=\"name_vector\", p=2)\n",
    "\n",
    "tfidf_vectorizer = Pipeline(stages=[regexTokenizer, ngram_creator, tf_counter, idf_counter, l2_normalizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoded_names = tfidf_vectorizer.fit(country_opr).transform(country_opr).select(['name_index', 'name_vector'])\n",
    "print(\"dummy_id:\\t row number\\nname_vector:\\t (nr_ngrams,[indices], [values])\")\n",
    "encoded_names.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all index comparisons to be made\n",
    "\n",
    "Transform to a dataframe which contains a column with the name index another with the n-gram index and a third one with the value given by the tfidf-normalizer transformation above.\n",
    "\n",
    "1. Unpack sparse vector into a lit of tuples containing (index, value) for value bigger than 0.1\n",
    "2. Set each tuple value to its own row according to the correspoing name_index\n",
    "3. Separte these tuples into different columns\n",
    "4. Get columns with name index, ngram index and the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema_sparse = ArrayType(StructType([\n",
    "    StructField(\"ngram_index\", IntegerType(), False),\n",
    "    StructField(\"value\", FloatType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# @sf.udf(schema_sparse)\n",
    "def unpack_vector(sparse):\n",
    "    \"\"\"Combine indices and values into a tuples.\n",
    "    \n",
    "    For each value below 0.1 in the sparse vector we create a tuple and\n",
    "    then add these tuples into a single list. The tuple contains the\n",
    "    index and the value.  \n",
    "    \"\"\"\n",
    "    return ((int(index), float(value)) for index, value in zip(sparse.indices, sparse.values) if value > 0.1)\n",
    "\n",
    "udf_unpack_vector = sf.udf(unpack_vector, schema_sparse)\n",
    "\n",
    "\n",
    "names_vs_ngrams = (\n",
    "    encoded_names\n",
    "    .withColumn('explode', sf.explode(udf_unpack_vector(sf.col('name_vector'))))\n",
    "    .withColumn('ngram_index',  sf.col('explode').getItem('ngram_index'))\n",
    "    .withColumn('value',  sf.col('explode').getItem('value'))\n",
    "    .select('name_index', 'ngram_index', 'value')\n",
    ")\n",
    "\n",
    "names_vs_ngrams.persist()\n",
    "\n",
    "names_vs_ngrams.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary about the obtained values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_vs_ngrams.describe('value').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform into Local Scipy CSR matrix\n",
    "\n",
    "1. Load pyspark into pandas dataframe (this step might need lots of memory)\n",
    "2. Change column types to match c++ implementation\n",
    "2. Create CSR Scipy matrix using the index and the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = names_vs_ngrams.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.name_index = df.name_index.astype(np.int32)\n",
    "df.ngram_index = df.ngram_index.astype(np.int32)\n",
    "df.value = df.value.astype(np.float64)\n",
    "\n",
    "names_vs_ngrams.unpersist()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CSR compressed form matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csr_names_vs_ngrams = csr_matrix(\n",
    "    (df.value.values, (df.name_index.values, df.ngram_index.values)),\n",
    "    shape=(df.name_index.max() + 1, df.ngram_index.max() + 1),\n",
    "    dtype=np.float64)\n",
    "del df\n",
    "\n",
    "print(\"Matrix Shape:\\t\", csr_names_vs_ngrams.shape, \"=\", csr_names_vs_ngrams.shape[0] * csr_names_vs_ngrams.shape[1])\n",
    "print(\"NonZero:\\t\", csr_names_vs_ngrams.count_nonzero())\n",
    "print(\"Sparsity:\\t\", csr_names_vs_ngrams.count_nonzero() / (csr_names_vs_ngrams.shape[0] * csr_names_vs_ngrams.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast and parallelize\n",
    "\n",
    "1. Broadcast complete matrix\n",
    "2. Calculate maximum possible number of partitions (no single row matrix partitions, we set minimum of 500 rows)\n",
    "3. Parallelize chunks of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_T_bcast = sc.broadcast(csr_names_vs_ngrams.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_n_chunks = math.floor(csr_names_vs_ngrams.shape[0] / 500)\n",
    "max_n_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we parallelize we also send the row number as this will be important when trying to compare with the real names of getting their id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_chunks = min(56 * 4, max_n_chunks)\n",
    "\n",
    "chunk_size = math.ceil(csr_names_vs_ngrams.shape[0] / n_chunks)\n",
    "n_chunks = math.ceil(csr_names_vs_ngrams.shape[0] / chunk_size)\n",
    "chunks = [(csr_names_vs_ngrams[(i * chunk_size) : min((i + 1) * chunk_size, csr_names_vs_ngrams.shape[0])], i * chunk_size) for i in range(n_chunks)]\n",
    "m_distr = sc.parallelize(chunks, numSlices=n_chunks)\n",
    "\n",
    "print(\"Nr. Chunks:\\t\", len(chunks))\n",
    "print(\"Chunk Size:\", chunk_size, \"from:\", csr_names_vs_ngrams.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute sparse multiplication\n",
    "\n",
    "It is important to remember that the index for the matrices is always from 0 to the number of rows - 1, hence we need to pass the starting row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarity = m_distr.flatMap(lambda x: chunk_dot_limit(x[0],\n",
    "                                                       m_T_bcast.value,\n",
    "                                                       ntop=ntop,\n",
    "                                                       threshold=threshold,\n",
    "                                                       start_row=x[1]))\n",
    "\n",
    "sim_schema = StructType([\n",
    "    StructField(\"i\", IntegerType(), False),\n",
    "    StructField(\"j\", IntegerType(), False),\n",
    "    StructField(\"SIMILARITY\", FloatType(), False)\n",
    "])\n",
    "\n",
    "similarity = similarity.toDF(sim_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del csr_names_vs_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# group similarities with column i being the group id\n",
    "grouping_window = (Window\n",
    "                   .partitionBy('j')\n",
    "                   .orderBy(sf.asc('i')))\n",
    "# keep only the first entry sorted alphabetically\n",
    "grp_sim = (\n",
    "    similarity\n",
    "    .withColumn(\"rn\", sf.row_number().over(grouping_window))\n",
    "    .filter(sf.col(\"rn\") == 1)\n",
    "    .drop('rn')\n",
    ")\n",
    "# remove group ID from column j\n",
    "grp_similarity = grp_sim.join(\n",
    "    grp_sim.select('j').subtract(grp_sim.select('i')),\n",
    "    on='j', how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join matches with original information of the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matches = (\n",
    "    grp_similarity\n",
    "    .join(country_opr, grp_similarity['i'] == country_opr['name_index'],\n",
    "          how='left').drop('name_index')\n",
    "    .selectExpr('i', 'j', 'id as SOURCE_ID', 'SIMILARITY', 'name as SOURCE_NAME')\n",
    "    .join(country_opr, grp_similarity['j'] == country_opr['name_index'],\n",
    "          how='left').drop('name_index')\n",
    "    .withColumn('COUNTRY_CODE', sf.lit(country_code))\n",
    "    .selectExpr('COUNTRY_CODE', 'SOURCE_ID', 'id as TARGET_ID',\n",
    "                'SIMILARITY', 'SOURCE_NAME', 'name as TARGET_NAME')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "matches.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_T_bcast.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are there any similarities with itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches[matches['SOURCE_ID'] == matches['TARGET_ID']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are there any permuted similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(matches\n",
    " .withColumn('original', sf.concat('SOURCE_ID', 'TARGET_ID'))\n",
    " .withColumn('permuted', sf.concat('TARGET_ID', 'SOURCE_ID'))\n",
    " .filter(sf.col('original') == sf.col('permuted'))\n",
    " .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "summary of similarities found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches.describe('SIMILARITY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many matches per name?\n",
    "\n",
    "Ideally we would like this to be less than the set number of maximum number of similarities to find `ntop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The set maximum number of matches is:\", ntop)\n",
    "\n",
    "matches.groupBy(['SOURCE_ID', 'SOURCE_NAME']).count().sort('count', ascending=False).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matches for a short name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_off = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_small = matches.where(sf.size(sf.split('SOURCE_NAME', '')) < cut_off)\n",
    "matches_small.describe('SIMILARITY').show()\n",
    "matches_small.select('SIMILARITY', 'SOURCE_NAME', 'TARGET_NAME').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matches for long names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_big = matches.where(sf.size(sf.split('SOURCE_NAME', '')) > cut_off)\n",
    "matches_big.describe('SIMILARITY').show()\n",
    "matches_big.select('SIMILARITY', 'SOURCE_NAME', 'TARGET_NAME').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('TARGET_ID').orderBy(sf.desc('SIMILARITY'), sf.asc('SOURCE_ID'))\n",
    "window = Window.partitionBy('TARGET_ID').orderBy(sf.asc('SOURCE_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "single_matches = (matches\n",
    "                  .withColumn(\"rn\", sf.row_number().over(window))\n",
    "                  .filter(sf.col(\"rn\") == 1).drop('rn')\n",
    "                  .filter(sf.col('TARGET_ID') > sf.col('SOURCE_ID'))\n",
    "                  .sort('SOURCE_ID', ascending=True))\n",
    "single_matches.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = single_matches.join(single_matches.select('TARGET_ID').subtract(single_matches.select('SOURCE_ID')), on='TARGET_ID', how='inner')\n",
    "# res.show()\n",
    "res[res.SOURCE_NAME.like('%aarhus tec%') | res.TARGET_NAME.like('%aarhus tec%')].sort('SOURCE_ID').select('SOURCE_ID', 'TARGET_ID', 'SIMILARITY', 'SOURCE_NAME', 'TARGET_NAME').show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(res[(matches['SOURCE_NAME'] == 'aarhus tech') | (res['TARGET_NAME'] == 'aarhus tech')]\n",
    " .select('SOURCE_ID', 'TARGET_ID', 'SIMILARITY', 'SOURCE_NAME', 'TARGET_NAME')\n",
    ").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res[res['TARGET_ID'] == 'DK~MM-INIT-OPER~O~1368026'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res[res['SOURCE_ID'] == 'DK~MM-INIT-OPER~O~1368026'].show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
