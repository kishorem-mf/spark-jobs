{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mantain a persistent UUID for groups\n",
    "\n",
    "The idea is that there are already grouped operators and each group has a unique UUID. When new operator data comes in we need to match them with current grouped operators and add them to a group if necessary keeping the UUID persistent.\n",
    "\n",
    "The new input operator data can be of three forms:\n",
    "\n",
    " - Completely new and not yet in our records: In this case the operators should be added with a new unique UUID\n",
    " - Known operator with no change: In this case the operator could still be matched in the same group its already in or reassign to a new group.\n",
    " - Known operator with a change in its data: Operator should remain in the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"NameMatching_Notebook\")\n",
    "    .config('spark.dynamicAllocation.enabled', False)\n",
    "    .config('spark.executorEnv.PYTHON_EGG_CACHE', '/tmp')\n",
    "    .config('spark.executor.instances', 4)\n",
    "    .config('spark.executor.cores', 13)\n",
    "    .config('spark.executor.memory', '14g')\n",
    "    .config('spark.driver.memory', '7g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group should have a unique UUID. Then I need a function to create this unique ID for each completely new operators which doesn't match to any group (they will become their own group). Since by concatenating the columns like `COUNTRY_CODE~SOURCE~REF_OPERATOR_ID` gives a unique opertor ID we can use this to generate the group ID. I do this passing this ID to a hashing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def create_32_char_hash(string):\n",
    "    hash_id = hashlib.md5(string.encode(encoding='utf-8')).hexdigest()\n",
    "    return '-'.join([hash_id[:8], hash_id[8:12], hash_id[12:16], hash_id[16:]])\n",
    "\n",
    "# create udf for use in spark later\n",
    "udf_create_32_char_hash = sf.udf(create_32_char_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need functions to preprocess the input operator data and the existing groupped operator data. \n",
    "\n",
    "First I declare a regex made of unwanted characters (given by Roderik), this will be filtered out during the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DROP_CHARS = \"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\" \\\n",
    "             \"\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\" \\\n",
    "             \"\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\" \\\n",
    "             \"\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\" \\\n",
    "             \"\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\" \\\n",
    "             \"\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\" \\\n",
    "             \"\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\" \\\n",
    "             \"\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\" \\\n",
    "             \"\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "REGEX = \"[{}]\".format(DROP_CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocesing from known grouped operators is different than from new input operator data. Below I create the matching string for the group which will be used for matching againts new input operator data. I also add a row number columns which is necessary for the matching later to join back with the original information of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_grouped_kown_operators(ddf):\n",
    "    # Create string name used originally for the matching\n",
    "    w = Window.partitionBy('countryCode').orderBy(sf.asc('ohubOperatorId'))\n",
    "    return (ddf\n",
    "            .withColumn('matching_string', sf.concat_ws(' ',\n",
    "                                                        sf.col('operator').getItem('nameCleansed'),\n",
    "                                                        sf.col('operator').getItem('cityCleansed'),\n",
    "                                                        sf.col('operator').getItem('streetCleansed'),\n",
    "                                                        sf.col('operator').getItem('zipCodeCleansed')))\n",
    "            .withColumn('matching_string', sf.regexp_replace('matching_string', REGEX, ''))\n",
    "            .withColumn('matching_string', sf.lower(sf.trim(sf.regexp_replace(sf.col('matching_string'), '\\s+', ' '))))\n",
    "            .withColumn('string_index', sf.row_number().over(w) - 1)\n",
    "            .select('countryCode', 'string_index', 'ohubOperatorId', 'matching_string'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing of known operators consist of making the unique refId and the string to be matched on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_input_operatros(ddf):\n",
    "    w = Window.partitionBy('COUNTRY_CODE').orderBy(sf.asc('refId'))\n",
    "    return (ddf\n",
    "            .na.drop(subset=['NAME_CLEANSED'])\n",
    "            .withColumn('refId', sf.concat_ws('~',\n",
    "                                   sf.col('COUNTRY_CODE'),\n",
    "                                   sf.col('SOURCE'),\n",
    "                                   sf.col('REF_OPERATOR_ID')))\n",
    "            .fillna('')\n",
    "            # create string columns to matched\n",
    "            .withColumn('matching_string',\n",
    "                        sf.concat_ws(' ',\n",
    "                                     sf.col('NAME_CLEANSED'),\n",
    "                                     sf.col('CITY_CLEANSED'),\n",
    "                                     sf.col('STREET_CLEANSED'),\n",
    "                                     sf.col('ZIP_CODE_CLEANSED')))\n",
    "            .withColumn('matching_string', sf.regexp_replace('matching_string', REGEX, ''))\n",
    "            .withColumn('matching_string', sf.lower(sf.trim(sf.regexp_replace('matching_string', '\\s+', ' '))))\n",
    "            .withColumn('string_index', sf.row_number().over(w) - 1)\n",
    "            .select('COUNTRY_CODE', 'string_index', 'refId', 'matching_string')\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all operators\n",
    "\n",
    "At the end of the matching I will need to join back the matches to the current operators to have the complete data of known and new input operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "operators_old_dir = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/test/OPERATORS_MERGED.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_oprs = (\n",
    "    spark\n",
    "    .read.parquet(operators_old_dir)\n",
    "    .withColumn('refId', sf.explode('refIds'))\n",
    "    .drop('refIds')\n",
    ")\n",
    "all_oprs.persist()\n",
    "all_oprs.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess grouped current operator data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "operators_old_dir = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/test/OPERATORS_MERGED.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oprs_old = preprocess_grouped_kown_operators(spark.read.parquet(operators_old_dir))\n",
    "\n",
    "oprs_old.persist()\n",
    "oprs_old.sort('string_index').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess new input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_operators_dir = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/test/OPERATORS.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_oprs = preprocess_input_operatros(spark.read.parquet(input_operators_dir))\n",
    "\n",
    "input_oprs.persist()\n",
    "input_oprs.sort('string_index').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match new input vs group operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Know that the format is ready to match we use the matching algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "egg_file = glob(os.path.join('..', 'dist', '*.egg'))[0]\n",
    "sc.addPyFile(egg_file)\n",
    "\n",
    "from string_matching.spark_string_matching import match_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_codes = (input_oprs\n",
    "                 .select('COUNTRY_CODE')\n",
    "                 .distinct()\n",
    "                 .rdd.map(lambda r: r[0]).collect())\n",
    "country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_code = 'DK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_top = 1 # we get only the top match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for country_code in country_codes:\n",
    "    print('\\nStart:', country_code)\n",
    "    \n",
    "    input_oprs_ctr = input_oprs.filter(sf.col('COUNTRY_CODE') == country_code).repartition('refId')\n",
    "    oprs_old_ctr = oprs_old.filter(sf.col('countryCode') == country_code).repartition('ohubOperatorId')\n",
    "\n",
    "    print('Calculating similarity')\n",
    "    \n",
    "    similarity = match_strings(\n",
    "        spark,\n",
    "        input_oprs_ctr.select('string_index', 'matching_string'),\n",
    "        df2=oprs_old_ctr.select('string_index', 'matching_string'),\n",
    "        string_column='matching_string', row_number_column='string_index',\n",
    "        n_top=n_top, threshold=0.8, n_gram=2, min_document_frequency=2, max_vocabulary_size=2000\n",
    "    )\n",
    "\n",
    "    print('Join to original refId and ohubOperatorId')\n",
    "\n",
    "    matches = (\n",
    "        input_oprs_ctr\n",
    "        .join(similarity, input_oprs_ctr['string_index'] == similarity['i'], how='left')\n",
    "        .drop('string_index')\n",
    "        .selectExpr('j', 'SIMILARITY',\n",
    "                    'matching_string as matching_string_input', 'refId')\n",
    "        .join(oprs_old_ctr, sf.col('j') == oprs_old_ctr['string_index'], how='left')\n",
    "        .drop('string_index')\n",
    "        .withColumn('countryCode', sf.lit(country_code))\n",
    "        .selectExpr('SIMILARITY',\n",
    "                    'countryCode',\n",
    "                    'matching_string_input',\n",
    "                    'matching_string as matching_string_old',\n",
    "                    'refId',\n",
    "                    'ohubOperatorId as ohubOperatorId_matched')\n",
    "    )\n",
    "    matches.persist()\n",
    "    matches.count()\n",
    "\n",
    "    print('Updating UUID')\n",
    "\n",
    "    updated_matching = (\n",
    "        matches\n",
    "        .join(all_oprs.filter(sf.col('countryCode') == country_code), on=['refId', 'countryCode'], how='outer')\n",
    "        .withColumn('ohubOperatorId',\n",
    "            sf.when(sf.col('ohubOperatorId_matched').isNotNull(), sf.col('ohubOperatorId_matched')).otherwise(sf.col('ohubOperatorId')))\n",
    "        .withColumn('ohubOperatorId', \n",
    "            sf.when(sf.col('ohubOperatorId').isNull(), udf_create_32_char_hash(sf.col('refId'))).otherwise(sf.col('ohubOperatorId')))\n",
    "        .groupBy('ohubOperatorId', 'operator', 'countryCode').agg(sf.collect_list('refId').alias('refIds'))\n",
    "    )\n",
    "    updated_matching.persist()\n",
    "    print('Number of groups:', oprs_old_ctr.count(), '-->', updated_matching.count())\n",
    "    \n",
    "    print('Writing to parquet')\n",
    "    \n",
    "    (updated_matching\n",
    "     .coalesce(20)\n",
    "     .write\n",
    "     .partitionBy('countryCode')\n",
    "     .parquet('adl://ulohubdldevne.azuredatalakestore.net/data/parquet/test/OPERATORS_MERGED_t2.parquet', mode='append'))\n",
    "    \n",
    "    print('Done:', country_code)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
