{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from glob import glob\n",
    "from os import path\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ../compile_library.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_file = glob(os.path.join('..', 'dist', '*.egg'))[0]\n",
    "egg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Contacts_Matching_Notebook\")\n",
    "#          .config('spark.dynamicAllocation.enabled', False)\n",
    "         .config('spark.executorEnv.PYTHON_EGG_CACHE', '/tmp')\n",
    "#          .config('spark.executor.instances', 4)\n",
    "#          .config('spark.executor.cores', 13)\n",
    "#          .config('spark.executor.memory', '14g')\n",
    "         .config('spark.driver.memory', '6g')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop = 10\n",
    "threshold = 0.7\n",
    "\n",
    "fraction = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/CONTACTPERSONS.parquet'\n",
    "output_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/CONTACTPERSONS_MATCHED.parquet'\n",
    "\n",
    "input_file = '../../data/CONTACTPERSONS.parquet'\n",
    "output_file = '../../data/CONTACTPERSONS_MATCHED.parquet'\n",
    "\n",
    "save_output = False\n",
    "mode = 'append'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_chars = \"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "regex = \"[{}]\".format(drop_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(input_file).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Driver Memory: \", sc._conf.get('spark.driver.memory'))\n",
    "\n",
    "w = Window.partitionBy('COUNTRY_CODE').orderBy(sf.asc('id'))\n",
    "\n",
    "all_contacts = (\n",
    "    spark\n",
    "    .read.parquet(input_file)\n",
    "    .sample(False, fraction)\n",
    "    # keep only if no email nor phone\n",
    "    .filter(sf.isnull(sf.col('EMAIL_ADDRESS')) & sf.isnull(sf.col('MOBILE_PHONE_NUMBER')))\n",
    "    # drop if no first name and no last name\n",
    "    .na.drop(subset=['FIRST_NAME_CLEANSED', 'LAST_NAME_CLEANSED'], how='all')\n",
    "    # drop if no street\n",
    "    .na.drop(subset=['STREET_CLEANSED'], how='any')\n",
    "    # same logic but for an empty string\n",
    "    .filter((sf.trim(sf.col('STREET_CLEANSED')) != '') &\n",
    "            ((sf.trim(sf.col('FIRST_NAME_CLEANSED')) != '') | (sf.trim(sf.col('LAST_NAME_CLEANSED')) != '')))\n",
    "    # create unique ID\n",
    "    .withColumn('id', sf.concat_ws('~',\n",
    "                                   sf.col('COUNTRY_CODE'),\n",
    "                                   sf.col('SOURCE'),\n",
    "                                   sf.col('REF_CONTACT_PERSON_ID')))\n",
    "    .fillna('')\n",
    "    # create string columns to matched\n",
    "    .withColumn('name',\n",
    "                sf.concat_ws(' ',\n",
    "                             sf.col('FIRST_NAME_CLEANSED'),\n",
    "                             sf.col('LAST_NAME_CLEANSED')))\n",
    "    .withColumn('name', sf.regexp_replace('name', regex, ''))\n",
    "    .withColumn('name', sf.trim(sf.regexp_replace('name', '\\s+', ' ')))\n",
    "    .withColumn('name_index', sf.row_number().over(w) - 1)\n",
    "    .select('name_index', 'id', 'name', 'COUNTRY_CODE', 'FIRST_NAME_CLEANSED', 'LAST_NAME_CLEANSED', 'STREET_CLEANSED', 'HOUSENUMBER', 'ZIP_CODE_CLEANSED', 'CITY_CLEANSED')\n",
    ")\n",
    "all_contacts.persist()\n",
    "all_contacts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_contacts_count = all_contacts.groupby('COUNTRY_CODE', 'name').count().sort('count', ascending=False)\n",
    "all_contacts_count.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "contacts_count = all_contacts.groupby('COUNTRY_CODE').count()\n",
    "contacts_count.sort('count', ascending=True).show(15)\n",
    "contacts_count.sort('count', ascending=False).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = (contacts_count[contacts_count['count'] > 100]\n",
    "                 .select('COUNTRY_CODE')\n",
    "                 .distinct()\n",
    "                 .rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "print(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string_matching.spark_string_matching import match_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for country_code in country_codes:\n",
    "    print('Start:', country_code)\n",
    "    \n",
    "    contacts = (\n",
    "        all_contacts\n",
    "        .filter(sf.col('COUNTRY_CODE') == country_code)\n",
    "        .drop('COUNTRY_CODE')\n",
    "        .repartition('id')\n",
    "        .sort('id', ascending=True)\n",
    "    )\n",
    "    \n",
    "    print('Number of contacts:', contacts.count())\n",
    "\n",
    "    print('Vectorizing names')\n",
    "    similarity = match_strings(spark, contacts,\n",
    "                               string_column='name', row_number_column='name_index',\n",
    "                               n_top=ntop, threshold=threshold, n_gram=2, min_document_frequency=2, max_vocabulary_size=1500)\n",
    "    similarity.persist()\n",
    "    print('Nr. of similarities:', similarity.count())\n",
    "    \n",
    "    matches = (\n",
    "        similarity\n",
    "        .join(contacts, similarity['i'] == contacts['name_index'],\n",
    "              how='left').drop('name_index')\n",
    "        .selectExpr('i', 'j', 'id as SOURCE_ID',\n",
    "                    'SIMILARITY', 'name as SOURCE_NAME',\n",
    "                    'STREET_CLEANSED as SOURCE_STREET',\n",
    "                    'ZIP_CODE_CLEANSED as SOURCE_ZIP_CODE', 'CITY_CLEANSED as SOURCE_CITY')\n",
    "        .join(contacts, similarity['j'] == contacts['name_index'],\n",
    "              how='left').drop('name_index')\n",
    "        .withColumn('COUNTRY_CODE', sf.lit(country_code))\n",
    "        .selectExpr('i', 'j', 'COUNTRY_CODE', 'SOURCE_ID',\n",
    "                    'id as TARGET_ID','SIMILARITY',\n",
    "                    'SOURCE_NAME', 'STREET_CLEANSED as TARGET_STREET',\n",
    "                    'SOURCE_STREET', 'name as TARGET_NAME',\n",
    "                    'SOURCE_ZIP_CODE', 'ZIP_CODE_CLEANSED as TARGET_ZIP_CODE',\n",
    "                    'SOURCE_CITY', 'CITY_CLEANSED as TARGET_CITY')\n",
    "        .filter((sf.col('SOURCE_ZIP_CODE') == sf.col('TARGET_ZIP_CODE')) | (sf.col('SOURCE_CITY') == sf.col('TARGET_CITY')))\n",
    "        .withColumn('street_lev_distance', sf.levenshtein(sf.col('SOURCE_STREET'), sf.col('TARGET_STREET')))\n",
    "    )\n",
    "    matches.persist()\n",
    "    print('Nr. of matches same city or Zipcode', matches.count())\n",
    "    \n",
    "    matches_lev = matches.filter(sf.col('street_lev_distance') < 5)\n",
    "    \n",
    "    print('Nr. of matches Levenshtein filter:', matches_lev.count())\n",
    "    \n",
    "    grouping_window = (\n",
    "    Window\n",
    "    .partitionBy('j')\n",
    "    .orderBy(sf.asc('i')))\n",
    "\n",
    "    # keep only the first entry sorted alphabetically\n",
    "    grp_match = (\n",
    "        matches_lev\n",
    "        .withColumn(\"rn\", sf.row_number().over(grouping_window))\n",
    "        .filter(sf.col(\"rn\") == 1)\n",
    "        .drop('rn')\n",
    "    )\n",
    "\n",
    "    # remove group ID from column j\n",
    "    grouped_matches =  grp_match.join(\n",
    "        grp_match.select('j').subtract(grp_match.select('i')),\n",
    "        on='j', how='inner'\n",
    "    )\n",
    "\n",
    "     \n",
    "    if save_output:\n",
    "        (grouped_matches\n",
    "         .coalesce(5)\n",
    "         .write\n",
    "         .partitionBy('country_code')\n",
    "         .parquet(output_file, mode=mode))\n",
    "    else:\n",
    "        grouped_matches.persist()\n",
    "        n_matches = grouped_matches.count()\n",
    "\n",
    "        print('\\n\\nNr. grouped matches:\\t', n_matches)\n",
    "        print('Threshold:\\t', threshold)\n",
    "        print('NTop:\\t', ntop)\n",
    "        (grouped_matches\n",
    "         .select('SIMILARITY',\n",
    "                 'SOURCE_NAME', 'TARGET_NAME',\n",
    "                 'SOURCE_STREET', 'TARGET_STREET',\n",
    "                 'SOURCE_ZIP_CODE', 'TARGET_ZIP_CODE',\n",
    "                 'SOURCE_CITY', 'TARGET_CITY')\n",
    "         .sort('SIMILARITY', ascending=True)\n",
    "         .show(50, truncate=False))\n",
    "\n",
    "        (grouped_matches\n",
    "         .groupBy(['SOURCE_ID', 'SOURCE_NAME'])\n",
    "         .count()\n",
    "         .sort('count', ascending=False).show(50, truncate=False))\n",
    "\n",
    "        grouped_matches.describe('SIMILARITY').show()\n",
    "    print(\"Done, country code:\", country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
