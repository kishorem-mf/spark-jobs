{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from glob import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path pointing to cython sparse_dot directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! bash /home/rodrigo/projects/unilever/ohub2/name-matching/compile_library.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = path.join('/home/rodrigo/projects/unilever/ohub2/name-matching', 'cython')\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_file = glob(path.join(cwd, 'dist', '*.egg'))[0]\n",
    "egg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"NameMatching-OUniverse\")\n",
    "#          .config('spark.dynamicAllocation.enabled', False)\n",
    "         .config('spark.executorEnv.PYTHON_EGG_CACHE', '/tmp')\n",
    "#          .config('spark.executor.instances', 4)\n",
    "#          .config('spark.executor.cores', 13)\n",
    "#          .config('spark.executor.memory', '14g')\n",
    "         .config('spark.driver.memory', '7g')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sparse_dot_topn.sparse_dot_topn as ct # this is the cython code module\n",
    "\n",
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "\n",
    "from pyspark.ml.param.shared import HasInputCol\n",
    "from pyspark.ml.param.shared import HasOutputCol\n",
    "from pyspark.ml.param.shared import Param\n",
    "from pyspark.ml.param.shared import Params\n",
    "\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType, FloatType\n",
    "from pyspark.mllib.linalg import VectorUDT, Vector, Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.functions import array, struct\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.sparse import _sparsetools\n",
    "from scipy.sparse.sputils import get_index_dtype\n",
    "import math\n",
    "from itertools import tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dot_limit(A, B, ntop, threshold=0, start_row=0):\n",
    "    import sparse_dot_topn.sparse_dot_topn as ct\n",
    "    B = B.tocsr()\n",
    "\n",
    "    M = A.shape[0]\n",
    "    N = B.shape[1]\n",
    "\n",
    "    idx_dtype = np.int32\n",
    "    \n",
    "    only_upper_triangular = 0\n",
    "\n",
    "    # massive memory reduction\n",
    "    # max number of possible non-zero element in the upper triangular matrix\n",
    "    nnz_max = M * ntop\n",
    "\n",
    "    # arrays will be returned by reference\n",
    "    rows = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    cols = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    data = np.empty(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    # C++ wrapped with Cython implementation\n",
    "    # number of found non-zero entries in the upper triangular matrix\n",
    "    # I'll use this value to slice the returning numpy array\n",
    "    nnz = ct.sparse_dot_topn(\n",
    "        M, N,\n",
    "        np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        threshold,\n",
    "        rows, cols, data, start_row, only_upper_triangular)\n",
    "#     return [(nnz, nnz, float(nnz))]\n",
    "    return ((int(i), int(j), float(v)) for i, j, v in\n",
    "            zip(rows[:nnz], cols[:nnz], data[:nnz]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameVectorizer(object):\n",
    "    def __init__(self, n_gram, min_df, vocab_size):\n",
    "        self.n_gram = n_gram\n",
    "        self.min_df = min_df\n",
    "        self.vocab_size = vocab_size\n",
    "        self.__create_pipeline()\n",
    "\n",
    "    def __create_pipeline(self):\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"name\",\n",
    "                                        outputCol=\"tokens\",\n",
    "                                        pattern=\"\")\n",
    "        ngram_creator = NGram(inputCol=\"tokens\",\n",
    "                              outputCol=\"n_grams\",\n",
    "                              n=self.n_gram)\n",
    "        tf_counter = CountVectorizer(inputCol='n_grams',\n",
    "                                          outputCol='term_frequency',\n",
    "                                          minTF=1.0,\n",
    "                                          minDF=self.min_df,\n",
    "                                          vocabSize=self.vocab_size,\n",
    "                                          binary=False)\n",
    "        idf_counter = IDF(inputCol=\"term_frequency\",\n",
    "                               outputCol=\"tfidf_vector\")\n",
    "        l2_normalizer = Normalizer(inputCol=\"tfidf_vector\",\n",
    "                                   outputCol=\"name_vector\",\n",
    "                                   p=2)\n",
    "        \n",
    "        self.pipeline = Pipeline(\n",
    "            stages=[regexTokenizer,\n",
    "                    ngram_creator,\n",
    "                    tf_counter,\n",
    "                    idf_counter,\n",
    "                    l2_normalizer]\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, df1, df2):\n",
    "        df = df1.union(df2)\n",
    "        self.pipeline = self.pipeline.fit(df)      \n",
    "        return self.pipeline.transform(df1), self.pipeline.transform(df2)\n",
    "\n",
    "\n",
    "def unpack_vector(sparse):\n",
    "    return ((int(index), float(value)) for index, value in\n",
    "            zip(sparse.indices, sparse.values) if value > 0.05)\n",
    "\n",
    "\n",
    "schema = StructType([StructField(\"dummy_id\", LongType(), False),\n",
    "                     StructField(\"id\", StringType(), False),\n",
    "                     StructField(\"name\", StringType(), False)])\n",
    "\n",
    "schema_sparse = ArrayType(StructType([\n",
    "    StructField(\"j_ngram\", IntegerType(), False),\n",
    "    StructField(\"value\", FloatType(), False)\n",
    "]))\n",
    "\n",
    "sim_schema = StructType([\n",
    "    StructField(\"i\", IntegerType(), False),\n",
    "    StructField(\"j\", IntegerType(), False),\n",
    "    StructField(\"SIMILARITY\", FloatType(), False)\n",
    "])\n",
    "\n",
    "udf_unpack_vector = sf.udf(unpack_vector, schema_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_opr1_csv(spark, filepath):\n",
    "    return(spark\n",
    "           .read.csv(filepath, header=True, sep=',')\n",
    "           .select('OPR_COUNTRY_CODE', 'OPR_NAME', 'OPR_REGION',\n",
    "                   'OPR_STREET', 'OPR_HOUSE_NUMBER', 'OPR_ZIP_CODE',\n",
    "                   'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID'))\n",
    "\n",
    "def read_opr2_csv(spark, filepath):\n",
    "    return(spark\n",
    "           .read.format(\"com.databricks.spark.avro\")\n",
    "           .load(filepath)\n",
    "           .select('inputCountry', 'name', 'formattedAddress', 'placeId'))\n",
    "    \n",
    "\n",
    "def preprocess_phase1(ddf):\n",
    "    w = Window.partitionBy('OPR_COUNTRY_CODE').orderBy(sf.asc('name'))\n",
    "    return (ddf\n",
    "            .fillna('')\n",
    "            # create string columns to matched\n",
    "            .withColumn('name',\n",
    "                        sf.concat_ws(' ',\n",
    "                                     sf.col('OPR_NAME'),\n",
    "                                     sf.col('OPR_REGION'),\n",
    "                                     sf.col('OPR_STREET'),\n",
    "                                     sf.col('OPR_HOUSE_NUMBER'),\n",
    "                                     sf.col('OPR_ZIP_CODE')))\n",
    "            .withColumn('name', sf.regexp_replace('name', REGEX, ''))\n",
    "            .withColumn('name', sf.trim(sf.regexp_replace('name', '\\s+', ' ')))\n",
    "            .filter(~sf.col('name').isin(*DROP_NAMES))\n",
    "            .withColumn('name_index', sf.row_number().over(w) - 1)\n",
    "            .selectExpr('name_index', 'name', 'OPR_COUNTRY_CODE as country_code', 'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID'))\n",
    "\n",
    "def preprocess_phase2(ddf):\n",
    "    w = Window.partitionBy('inputCountry').orderBy(sf.asc('name'))\n",
    "    return (ddf\n",
    "            .fillna('')\n",
    "            # create string columns to matched\n",
    "            .withColumn('name',\n",
    "                        sf.concat_ws(' ',\n",
    "                                     sf.col('name'),\n",
    "                                     sf.col('formattedAddress')))\n",
    "            .withColumn('name', sf.regexp_replace('name', REGEX, ''))\n",
    "            .withColumn('name', sf.trim(sf.regexp_replace('name', '\\s+', ' ')))\n",
    "            .filter(~sf.col('name').isin(*DROP_NAMES))\n",
    "            .withColumn('name_index', sf.row_number().over(w) - 1)\n",
    "            .selectExpr('name_index', 'name', 'inputCountry as country_code', 'placeId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phase_1_file = '../../Phase_I/Phase_I_Input_OHUB_Operator_Files/20181101/OPR_20180111.csv'\n",
    "phase_2_file = '../../Phase_II/Phase_II_Output/avro/*.avro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTOP = 3\n",
    "THRESHOLD = 0.7\n",
    "MATRIX_CHUNK_ROWS = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_chars = \"\\\"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "REGEX = \"[{}]\".format(drop_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_NAMES = ['unknown', '', ' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_opr_raw = read_opr1_csv(spark, phase_1_file)\n",
    "count_phase_1_raw = phase1_opr_raw.groupby('OPR_COUNTRY_CODE').count()\n",
    "\n",
    "phase1_opr = preprocess_phase1(phase1_opr_raw)\n",
    "phase1_opr.persist()\n",
    "count_phase_1 = phase1_opr.groupby('country_code').count()\n",
    "\n",
    "phase1_opr.show(5, truncate=False)\n",
    "\n",
    "count_phase_1_raw.join(count_phase_1, sf.col('OPR_COUNTRY_CODE') == sf.col('country_code')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phase2_opr_raw = read_opr2_csv(spark, phase_2_file)\n",
    "count_phase_2_raw = phase2_opr_raw.groupby('inputCountry').count()\n",
    "\n",
    "phase2_opr = preprocess_phase2(phase2_opr_raw)\n",
    "phase2_opr.persist()\n",
    "count_phase_2 = phase2_opr.groupby('country_code').count()\n",
    "\n",
    "phase2_opr.show(5, truncate=False)\n",
    "\n",
    "count_phase_2_raw.join(count_phase_2, sf.col('inputCountry') == sf.col('country_code')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = (count_phase_1\n",
    "                 .select('country_code')\n",
    "                 .distinct()\n",
    "                 .rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "print(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_repartition_country(ddf, country_code):\n",
    "    return (ddf\n",
    "            .filter(sf.col('country_code') == country_code)\n",
    "            .drop('country_code')\n",
    "            .repartition('name_index')\n",
    "            .sort('name_index', ascending=True))\n",
    "\n",
    "def dense_to_sparse_ddf(ddf):\n",
    "    udf_unpack_vector = sf.udf(unpack_vector, ngram_schema)\n",
    "    return (ddf\n",
    "            .withColumn('explode', sf.explode(udf_unpack_vector(sf.col('name_vector'))))\n",
    "            .withColumn('ngram_index', sf.col('explode').getItem('ngram_index'))\n",
    "            .withColumn('value', sf.col('explode').getItem('value'))\n",
    "            .select('name_index', 'ngram_index', 'value'))\n",
    "\n",
    "def sparse_to_csr_matrix(ddf):\n",
    "    df = ddf.toPandas()\n",
    "    df.name_index = df.name_index.astype(np.int32)\n",
    "    df.ngram_index = df.ngram_index.astype(np.int32)\n",
    "    df.value = df.value.astype(np.float64)\n",
    "\n",
    "    csr_names_vs_ngrams = csr_matrix(\n",
    "        (df.value.values, (df.name_index.values, df.ngram_index.values)),\n",
    "        shape=(df.name_index.max() + 1, df.ngram_index.max() + 1),\n",
    "        dtype=np.float64)\n",
    "    del df\n",
    "    return csr_names_vs_ngrams\n",
    "\n",
    "def split_into_chunks(csr_names_vs_ngrams):\n",
    "    n_chunks = max(1, math.floor(csr_names_vs_ngrams.shape[0] / MATRIX_CHUNK_ROWS))\n",
    "    chunk_size = math.ceil(csr_names_vs_ngrams.shape[0] / n_chunks)\n",
    "    print(\"Matrix chunk size is \" + str(chunk_size))\n",
    "    n_chunks = math.ceil(csr_names_vs_ngrams.shape[0] / chunk_size)\n",
    "    chunks = [(csr_names_vs_ngrams[\n",
    "               (i * chunk_size): min((i + 1) * chunk_size, csr_names_vs_ngrams.shape[0])], i * chunk_size)\n",
    "              for i in range(n_chunks)]\n",
    "    return chunks\n",
    "\n",
    "ngram_schema = ArrayType(StructType([\n",
    "    StructField(\"ngram_index\", IntegerType(), False),\n",
    "    StructField(\"value\", FloatType(), False)\n",
    "]))\n",
    "\n",
    "similarity_schema = StructType([\n",
    "    StructField(\"i\", IntegerType(), False),\n",
    "    StructField(\"j\", IntegerType(), False),\n",
    "    StructField(\"SIMILARITY\", FloatType(), False)\n",
    "])\n",
    "\n",
    "def calculate_similarity(chunks_rdd, csr_rdd_transpose):\n",
    "    similarity = chunks_rdd.flatMap(\n",
    "        lambda x: chunk_dot_limit(x[0], csr_rdd_transpose.value,\n",
    "                                  ntop=NTOP,\n",
    "                                  threshold=THRESHOLD,\n",
    "                                  start_row=x[1])\n",
    "    )\n",
    "\n",
    "    return similarity.toDF(similarity_schema)\n",
    "\n",
    "\n",
    "def find_matches(similarity, opr_1, opr_2, country_code):\n",
    "    return (similarity\n",
    "            .join(opr_1, similarity['i'] == opr_1['name_index'],\n",
    "                  how='left').drop('name_index')\n",
    "            .selectExpr('i', 'j', 'SIMILARITY', 'name as name_phase1',\n",
    "                        'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID')\n",
    "            .join(opr_2, similarity['j'] == opr_2['name_index'],\n",
    "                  how='left').drop('name_index')\n",
    "            .withColumn('COUNTRY_CODE', sf.lit(country_code))\n",
    "            .selectExpr('COUNTRY_CODE', 'SIMILARITY', 'name_phase1', 'name as name_phase2', \n",
    "                        'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID', 'placeId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for country_code in country_codes:\n",
    "    print(country_code, 'START')\n",
    "    \n",
    "    ctr_opr1 = select_and_repartition_country(phase1_opr, country_code)\n",
    "    ctr_opr2 = select_and_repartition_country(phase2_opr, country_code)\n",
    "\n",
    "    encoded_names_1, encoded_names_2 = NameVectorizer(n_gram=2, min_df=2, vocab_size=1000).fit_transform(ctr_opr1.select('name_index', 'name'),\n",
    "                                                                                                         ctr_opr2.select('name_index', 'name'))\n",
    "\n",
    "    names_vs_ngrams_1 = dense_to_sparse_ddf(encoded_names_1)\n",
    "    names_vs_ngrams_2 = dense_to_sparse_ddf(encoded_names_2)\n",
    "\n",
    "    csr_names_vs_ngrams_1 = sparse_to_csr_matrix(names_vs_ngrams_1)\n",
    "    csr_names_vs_ngrams_2 = sparse_to_csr_matrix(names_vs_ngrams_2)\n",
    "\n",
    "    csr_rdd_2 = spark.sparkContext.broadcast(csr_names_vs_ngrams_2.transpose())\n",
    "\n",
    "    chunks = split_into_chunks(csr_names_vs_ngrams_1)\n",
    "    print(\"Parallelizing matrix in \" + str(len(chunks)) + \" chunks\")\n",
    "    chunks_rdd = spark.sparkContext.parallelize(chunks, numSlices=len(chunks))\n",
    "\n",
    "    del csr_names_vs_ngrams_1, csr_names_vs_ngrams_2\n",
    "\n",
    "    similarity = calculate_similarity(chunks_rdd, csr_rdd_2)\n",
    "    # similarity.sort('SIMILARITY', ascending=False).show(10)\n",
    "\n",
    "    matches = find_matches(similarity, ctr_opr1, ctr_opr2, country_code)\n",
    "    # matches.sort('SIMILARITY', ascending=False).show(50, truncate=False)\n",
    "    \n",
    "    matches.coalesce(1).write.csv(country_code + '.csv', header=True)\n",
    "    print(country_code, 'DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:unilever]",
   "language": "python",
   "name": "conda-env-unilever-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
