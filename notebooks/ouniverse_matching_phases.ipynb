{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUniverse\n",
    "**Match Pase 1 Input vs Phase 2 output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from glob import glob\n",
    "from os import path\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path pointing to cython sparse_dot directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ../compile_library.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_file = glob(os.path.join('..', 'dist', '*.egg'))[0]\n",
    "egg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"NameMatching_Notebook\")\n",
    "#              .config('spark.dynamicAllocation.enabled', False)\n",
    "         .config('spark.executorEnv.PYTHON_EGG_CACHE', '/tmp')\n",
    "#              .config('spark.executor.instances', 4)\n",
    "#              .config('spark.executor.cores', 13)\n",
    "#              .config('spark.executor.memory', '14g')\n",
    "         .config('spark.driver.memory', '4g')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phase_1_file = '../../data/Phase_I/Phase_I_Input/20181101/OPERATORS_20180111.csv'\n",
    "phase_2_file = '../../data/Phase_II/Phase_II_Output/avro/*.avro'\n",
    "\n",
    "save_output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTOP = 3\n",
    "THRESHOLD = 0.5\n",
    "MATRIX_CHUNK_ROWS = 750\n",
    "FRACTION = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_chars = \"\\\"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "REGEX = \"[{}]\".format(drop_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_NAMES = ['unknown', '', ' ', 'zzdummy es080']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_opr1_csv(spark, filepath):\n",
    "    return(spark\n",
    "           .read.csv(filepath, header=True, sep=',')\n",
    "           .sample(False, FRACTION)\n",
    "           .select('OPR_COUNTRY_CODE', 'OPR_NAME', 'OPR_REGION',\n",
    "                   'OPR_STREET', 'OPR_HOUSE_NUMBER', 'OPR_ZIP_CODE',\n",
    "                   'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID'))\n",
    "\n",
    "def read_opr2_csv(spark, filepath):\n",
    "    return(spark\n",
    "           .read.format(\"com.databricks.spark.avro\")\n",
    "           .load(filepath)\n",
    "           .sample(False, FRACTION)\n",
    "           .select('inputCountry', 'name', 'formattedAddress', 'placeId'))\n",
    "    \n",
    "\n",
    "def preprocess_phase1(ddf):\n",
    "    w = Window.partitionBy('OPR_COUNTRY_CODE').orderBy(sf.asc('name'))\n",
    "    return (ddf\n",
    "            .fillna('')\n",
    "            # create string columns to matched\n",
    "            .withColumn('name',\n",
    "                        sf.concat_ws(' ',\n",
    "                                     sf.col('OPR_NAME'),\n",
    "                                     sf.col('OPR_REGION'),\n",
    "                                     sf.col('OPR_STREET'),\n",
    "                                     sf.col('OPR_HOUSE_NUMBER'),\n",
    "                                     sf.col('OPR_ZIP_CODE')))\n",
    "            .withColumn('name', sf.lower(sf.col('name')))\n",
    "            .withColumn('name', sf.regexp_replace(sf.col('name'), REGEX, ''))\n",
    "            .withColumn('name', sf.trim(sf.regexp_replace(sf.col('name'), '\\s+', ' ')))\n",
    "            .filter(~sf.col('name').isin(*DROP_NAMES))\n",
    "            .withColumn('name_index', sf.row_number().over(w) - 1)\n",
    "            .selectExpr('name_index', 'name', 'OPR_COUNTRY_CODE as country_code',\n",
    "                        'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID'))\n",
    "\n",
    "def preprocess_phase2(ddf):\n",
    "    w = Window.partitionBy('inputCountry').orderBy(sf.asc('name'))\n",
    "    return (ddf\n",
    "            .drop_duplicates(subset=['placeId'])\n",
    "            .fillna('')\n",
    "            # create string columns to matched\n",
    "            .withColumn('name',\n",
    "                        sf.concat_ws(' ',\n",
    "                                     sf.col('name'),\n",
    "                                     sf.col('formattedAddress')))\n",
    "            .withColumn('name', sf.lower(sf.col('name')))\n",
    "            .withColumn('name', sf.regexp_replace(sf.col('name'), REGEX, ''))\n",
    "            .withColumn('name', sf.trim(sf.regexp_replace(sf.col('name'), '\\s+', ' ')))\n",
    "            .filter(~sf.col('name').isin(*DROP_NAMES))\n",
    "            .withColumn('name_index', sf.row_number().over(w) - 1)\n",
    "            .selectExpr('name_index', 'name', 'inputCountry as country_code', 'placeId'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phase1_opr_raw = read_opr1_csv(spark, phase_1_file)\n",
    "count_phase_1_raw = phase1_opr_raw.groupby('OPR_COUNTRY_CODE').count().selectExpr('OPR_COUNTRY_CODE as country_code', 'count as count_raw')\n",
    "\n",
    "phase1_opr = preprocess_phase1(phase1_opr_raw)\n",
    "phase1_opr.persist()\n",
    "count_phase_1 = phase1_opr.groupby('country_code').count().selectExpr('country_code', 'count as count_clean')\n",
    "\n",
    "phase1_opr.select('name_index', 'name', 'OPR_SOURCE').show(5, truncate=False)\n",
    "\n",
    "(count_phase_1_raw\n",
    " .join(count_phase_1, on='country_code')\n",
    " .withColumn('percentage_kept', 100 * sf.col('count_clean') / sf.col('count_raw'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phase2_opr_raw = read_opr2_csv(spark, phase_2_file)\n",
    "count_phase_2_raw = phase2_opr_raw.groupby('inputCountry').count().selectExpr('inputCountry as country_code', 'count as count_raw')\n",
    "\n",
    "phase2_opr = preprocess_phase2(phase2_opr_raw)\n",
    "phase2_opr.persist()\n",
    "count_phase_2 = phase2_opr.groupby('country_code').count().selectExpr('country_code', 'count as count_clean')\n",
    "\n",
    "phase2_opr.show(5, truncate=False)\n",
    "\n",
    "(count_phase_2_raw\n",
    " .join(count_phase_2, on='country_code')\n",
    " .withColumn('percentage_kept', 100 * sf.col('count_clean') / sf.col('count_raw'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = (count_phase_1\n",
    "                 .select('country_code')\n",
    "                 .distinct()\n",
    "                 .rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "print(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_repartition_country(ddf, country_code):\n",
    "    return (ddf\n",
    "            .filter(sf.col('country_code') == country_code)\n",
    "            .drop('country_code')\n",
    "            .repartition('name_index')\n",
    "            .sort('name_index', ascending=True))\n",
    "\n",
    "\n",
    "def append_matches(similarity, opr_1, opr_2, country_code):\n",
    "    return (opr_1\n",
    "            .join(similarity, opr_1['name_index'] == similarity['i'],\n",
    "            how='left').drop('name_index')\n",
    "            .selectExpr('j', 'SIMILARITY', 'name as name_phase1',\n",
    "                        'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID')\n",
    "            .join(opr_2, sf.col('j') == opr_2['name_index'],\n",
    "            how='left').drop('name_index')\n",
    "            .withColumn('COUNTRY_CODE', sf.lit(country_code))\n",
    "            .selectExpr('COUNTRY_CODE', 'SIMILARITY', 'name_phase1', 'name as name_phase2', \n",
    "                        'OPR_SOURCE', 'OPR_OPR_ORIG_INTEGRATION_ID', 'placeId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string_matching.spark_string_matching import match_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for country_code in country_codes:\n",
    "    print(country_code, 'START')\n",
    "    \n",
    "    ctr_opr1 = select_and_repartition_country(phase1_opr, country_code)\n",
    "    ctr_opr2 = select_and_repartition_country(phase2_opr, country_code)\n",
    "   \n",
    "    similarity = match_strings(spark, ctr_opr1.select('name_index', 'name'),\n",
    "                               df2=ctr_opr2.select('name_index', 'name'),\n",
    "                               string_column='name', row_number_column='name_index',\n",
    "                               n_top=NTOP, threshold=THRESHOLD, n_gram=2, min_document_frequency=2, max_vocabulary_size=1500)\n",
    "    \n",
    "    \n",
    "    matches = append_matches(similarity, ctr_opr1, ctr_opr2, country_code)\n",
    "    \n",
    "    if save_output:\n",
    "        (matches\n",
    "         .coalesce(1)\n",
    "         .write\n",
    "         .csv(country_code + '.csv', header=True))\n",
    "        print('File saved for', country_code)\n",
    "    else:\n",
    "        matches.persist()\n",
    "        n_matches = matches.dropna(subset=['SIMILARITY']).count()\n",
    "\n",
    "        print('\\n\\nNr. Similarities:\\t', n_matches)\n",
    "        print('Threshold:\\t', THRESHOLD)\n",
    "        print('NTop:\\t', NTOP)\n",
    "        print('Fraction', FRACTION)\n",
    "        (matches\n",
    "         .select('SIMILARITY', 'name_phase1',\n",
    "                 'name_phase2', 'placeId')\n",
    "         .sample(False, 0.1)\n",
    "         .sort('SIMILARITY', ascending=False)\n",
    "         .show(50, truncate=True))\n",
    "\n",
    "        matches.describe('SIMILARITY').show()\n",
    "    \n",
    "    print(country_code, 'DONE\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
