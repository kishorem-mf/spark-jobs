{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from glob import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ../compile_library.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_file = glob(os.path.join('..', 'dist', '*.egg'))[0]\n",
    "egg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "spark = (pyspark.sql\n",
    "         .SparkSession\n",
    "         .builder\n",
    "         .appName(\"NameMatching_Notebook\")\n",
    "#              .config('spark.dynamicAllocation.enabled', False)\n",
    "         .config('spark.executorEnv.PYTHON_EGG_CACHE', '/tmp')\n",
    "#              .config('spark.executor.instances', 4)\n",
    "#              .config('spark.executor.cores', 13)\n",
    "#              .config('spark.executor.memory', '14g')\n",
    "         .config('spark.driver.memory', '7g')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS.parquet'\n",
    "output_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS_MATCHED.parquet'\n",
    "\n",
    "input_file = '../../data/OPERATORS.parquet'\n",
    "output_file = '../../data/OPERATORS_MATCHED.parquet'\n",
    "\n",
    "save_output = False\n",
    "mode = 'overwrite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop = 10\n",
    "threshold = 0.8\n",
    "fraction = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_chars = \"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "regex = \"[{}]\".format(drop_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Driver Memory: \", sc._conf.get('spark.driver.memory'))\n",
    "\n",
    "w = Window.partitionBy('COUNTRY_CODE').orderBy(sf.asc('id'))\n",
    "\n",
    "all_operators = (\n",
    "    spark\n",
    "    .read.parquet(input_file)\n",
    "    .sample(False, fraction)\n",
    "    .na.drop(subset=['NAME_CLEANSED'])\n",
    "    # create unique ID\n",
    "    .withColumn('id', sf.concat_ws('~',\n",
    "                                   sf.col('COUNTRY_CODE'),\n",
    "                                   sf.col('SOURCE'),\n",
    "                                   sf.col('REF_OPERATOR_ID')))\n",
    "    .fillna('')\n",
    "    # create string columns to matched\n",
    "    .withColumn('name',\n",
    "                sf.concat_ws(' ',\n",
    "                             sf.col('NAME_CLEANSED'),\n",
    "                             sf.col('CITY_CLEANSED'),\n",
    "                             sf.col('STREET_CLEANSED'),\n",
    "                             sf.col('ZIP_CODE_CLEANSED')))\n",
    "    .withColumn('name', sf.regexp_replace('name', regex, ''))\n",
    "    .withColumn('name', sf.trim(sf.regexp_replace('name', '\\s+', ' ')))\n",
    "    .withColumn('name_index', sf.row_number().over(w) - 1)\n",
    "    .select('name_index', 'id', 'name', 'COUNTRY_CODE')\n",
    ")\n",
    "all_operators.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_opr_count = all_operators.groupby('COUNTRY_CODE','name').count().sort('count', ascending=False)\n",
    "all_opr_count.show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opr_count = all_operators.groupby('COUNTRY_CODE').count()\n",
    "opr_count.sort('count', ascending=True).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = (opr_count[opr_count['count'] > 100]\n",
    "                 .select('COUNTRY_CODE')\n",
    "                 .distinct()\n",
    "                 .rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "print(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string_matching.spark_string_matching import match_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for country_code in country_codes:\n",
    "    operators = (\n",
    "        all_operators[sf.col('COUNTRY_CODE') == country_code]\n",
    "        .filter(sf.col('COUNTRY_CODE') == country_code)\n",
    "        .drop('COUNTRY_CODE')\n",
    "        .repartition('id')\n",
    "        .sort('id', ascending=True)\n",
    "    )\n",
    "    \n",
    "    similarity = match_strings(spark, operators,\n",
    "                               string_column='name', row_number_column='name_index',\n",
    "                               n_top=ntop, threshold=threshold, n_gram=2, min_document_frequency=2, max_vocabulary_size=1500)\n",
    "\n",
    "    grouping_window = (\n",
    "        Window\n",
    "        .partitionBy('j')\n",
    "        .orderBy(sf.asc('i')))\n",
    "\n",
    "    # keep only the first entry sorted alphabetically\n",
    "    grp_sim = (\n",
    "        similarity\n",
    "        .withColumn(\"rn\", sf.row_number().over(grouping_window))\n",
    "        .filter(sf.col(\"rn\") == 1)\n",
    "        .drop('rn')\n",
    "    )\n",
    "\n",
    "    # remove group ID from column j\n",
    "    grouped_similarity =  grp_sim.join(\n",
    "        grp_sim.select('j').subtract(grp_sim.select('i')),\n",
    "        on='j', how='inner'\n",
    "    )\n",
    "\n",
    "    matches = (\n",
    "        grouped_similarity\n",
    "        .join(operators, grouped_similarity['i'] == operators['name_index'],\n",
    "              how='left').drop('name_index')\n",
    "        .selectExpr('i', 'j', 'id as SOURCE_ID',\n",
    "                    'SIMILARITY', 'name as SOURCE_NAME')\n",
    "        .join(operators, grouped_similarity['j'] == operators['name_index'],\n",
    "              how='left').drop('name_index')\n",
    "        .withColumn('COUNTRY_CODE', sf.lit(country_code))\n",
    "        .selectExpr('COUNTRY_CODE', 'SOURCE_ID', 'id as TARGET_ID',\n",
    "                    'SIMILARITY', 'SOURCE_NAME', 'name as TARGET_NAME')\n",
    "    )\n",
    "\n",
    "    if save_output:\n",
    "        (matches\n",
    "         .coalesce(20)\n",
    "         .write\n",
    "         .partitionBy('country_code')\n",
    "         .parquet(fn, mode=mode))\n",
    "    else:\n",
    "        matches.persist()\n",
    "        n_matches = matches.count()\n",
    "\n",
    "        print('\\n\\nNr. Similarities:\\t', n_matches)\n",
    "        print('Threshold:\\t', threshold)\n",
    "        print('NTop:\\t', ntop)\n",
    "        (matches\n",
    "         .select('SOURCE_ID', 'TARGET_ID',\n",
    "                 'SIMILARITY', 'SOURCE_NAME', 'TARGET_NAME')\n",
    "         .sort('SIMILARITY', ascending=True)\n",
    "         .show(50, truncate=False))\n",
    "\n",
    "        (matches\n",
    "         .groupBy(['SOURCE_ID', 'SOURCE_NAME'])\n",
    "         .count()\n",
    "         .sort('count', ascending=False).show(50, truncate=False))\n",
    "\n",
    "        matches.describe('SIMILARITY').show()\n",
    "    print(\"Done, country code:\", country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
