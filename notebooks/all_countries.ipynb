{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from glob import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = path.join(os.getcwd(), 'cython')\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subprocess.run(['python', 'setup.py', 'bdist_egg'], cwd=cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "egg_file = glob(path.join('cython', 'dist', '*.egg'))[0]\n",
    "egg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('NameMatching')\n",
    "conf.setExecutorEnv('PYTHON_EGG_CACHE', '/tmp')\n",
    "# supposing you have 4 nodes with 8 cores each\n",
    "conf.set('spark.dynamicAllocation.enabled', False)\n",
    "conf.set('spark.executor.instances', 4) # low so that memory sharing is high\n",
    "conf.set('spark.executor.cores', 14)\n",
    "conf.set('spark.executor.memory', '14g')\n",
    "conf.set('spark.driver.memory', '15g')\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"NameMatching\", conf=conf)\n",
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sparse_dot_topn.sparse_dot_topn as ct # this is the cython code module\n",
    "\n",
    "from pyspark import keyword_only\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "\n",
    "from pyspark.ml.param.shared import HasInputCol\n",
    "from pyspark.ml.param.shared import HasOutputCol\n",
    "from pyspark.ml.param.shared import Param\n",
    "from pyspark.ml.param.shared import Params\n",
    "\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType, FloatType\n",
    "from pyspark.mllib.linalg import VectorUDT, Vector, Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.functions import array, struct\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.sparse import _sparsetools\n",
    "from scipy.sparse.sputils import get_index_dtype\n",
    "import math\n",
    "from itertools import tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunk_dot_limit(A, B, ntop, lower_bound=0, start_row=0):\n",
    "    B = B.tocsr()\n",
    "\n",
    "    M, K1 = A.shape\n",
    "    K2, N = B.shape\n",
    "\n",
    "    idx_dtype = np.int32\n",
    "\n",
    "    nnz_max = M * ntop\n",
    "\n",
    "    indptr = np.empty(M + 1, dtype=idx_dtype)\n",
    "    rows = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    cols = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    data = np.empty(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N,\n",
    "        np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, rows, cols, data)\n",
    "\n",
    "    max_k = indptr[-1]\n",
    "    return ((int(i) + start_row, int(j), float(v))\n",
    "            for i, j, v in zip(rows[:max_k], cols[:max_k], data[:max_k])\n",
    "            if (i + start_row) < j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NameVectorizer(object):\n",
    "\n",
    "    def __init__(self, n_gram, min_df, vocab_size):\n",
    "        self.n_gram = n_gram\n",
    "        self.min_df = min_df\n",
    "        self.vocab_size = vocab_size\n",
    "        self.__create_pipeline()\n",
    "\n",
    "    def __create_pipeline(self):\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"name\",\n",
    "                                        outputCol=\"tokens\",\n",
    "                                        pattern=\"\")\n",
    "        ngram_creator = NGram(inputCol=\"tokens\",\n",
    "                              outputCol=\"n_grams\",\n",
    "                              n=self.n_gram)\n",
    "        tf_counter = CountVectorizer(inputCol='n_grams',\n",
    "                                     outputCol='term_frequency',\n",
    "                                     minTF=1.0,\n",
    "                                     minDF=self.min_df,\n",
    "                                     vocabSize=self.vocab_size,\n",
    "                                     binary=False)\n",
    "        idf_counter = IDF(inputCol=\"term_frequency\",\n",
    "                          outputCol=\"tfidf_vector\")\n",
    "        l2_normalizer = Normalizer(inputCol=\"tfidf_vector\",\n",
    "                                   outputCol=\"encoded_vector\",\n",
    "                                   p=2)\n",
    "\n",
    "        self.pipeline = Pipeline(\n",
    "            stages=[regexTokenizer,\n",
    "                    ngram_creator,\n",
    "                    tf_counter,\n",
    "                    idf_counter,\n",
    "                    l2_normalizer]\n",
    "        )\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self.pipeline.fit(df)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return self.pipeline.transform(df)\n",
    "\n",
    "\n",
    "def zip_sparse(sparse):\n",
    "    return ((int(index), float(value))\n",
    "            for index, value in zip(sparse.indices, sparse.values)\n",
    "            if value > 0.1)\n",
    "\n",
    "\n",
    "schema = StructType([StructField(\"dummy_id\", LongType(), False),\n",
    "                     StructField(\"id\", StringType(), False),\n",
    "                     StructField(\"name\", StringType(), False)])\n",
    "\n",
    "schema_sparse = ArrayType(StructType([\n",
    "    StructField(\"j_ngram\", IntegerType(), False),\n",
    "    StructField(\"value\", FloatType(), False)\n",
    "]))\n",
    "\n",
    "sim_schema = StructType([\n",
    "    StructField(\"i\", IntegerType(), False),\n",
    "    StructField(\"j\", IntegerType(), False),\n",
    "    StructField(\"SIMILARITY\", FloatType(), False)\n",
    "])\n",
    "\n",
    "udf_zip_sparse = sf.udf(zip_sparse, schema_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS.parquet'\n",
    "output_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS_MATCHED.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntop = 100\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_chars = \"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "regex = \"[{}]\".format(drop_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_names = ['unknown', 'unknown companyad', 'not specified notspecified not specified0 00000', '是否 54534', 'privat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "print(\"Driver Memory: \", sc._conf.get('spark.driver.memory'))\n",
    "\n",
    "all_operators = (\n",
    "    sqlContext\n",
    "    .read.parquet(input_file)\n",
    "    .na.drop(subset=['NAME_CLEANSED'])\n",
    "    .withColumn('id',\n",
    "                sf.concat('COUNTRY_CODE', sf.lit(\"~\"),\n",
    "                          'SOURCE', sf.lit('~'),\n",
    "                          'REF_OPERATOR_ID'))\n",
    "    .fillna('')\n",
    "    .withColumn('name',\n",
    "                sf.concat('NAME_CLEANSED', sf.lit(' '),\n",
    "                          sf.col('CITY_CLEANSED'), sf.lit(' '),\n",
    "                          sf.col('STREET_CLEANSED'), sf.lit(' '),\n",
    "                          sf.col('ZIP_CODE_CLEANSED')))\n",
    "    .withColumn('name', sf.regexp_replace('name', regex, ''))\n",
    "    .withColumn('name', sf.trim(sf.regexp_replace('name', '\\s+', ' ')))\n",
    "    .filter(sf.col('name').isin(*drop_names) == False)\n",
    ")\n",
    "all_operators.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_opr_count = all_operators.groupby('COUNTRY_CODE','name').count().sort('count', ascending=False)\n",
    "all_opr_count.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_opr_count.show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opr_count = all_operators.groupby('COUNTRY_CODE').count()\n",
    "opr_count.sort('count', ascendint=True).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_codes = (opr_count[opr_count['count'] > 100]\n",
    "                 .select('COUNTRY_CODE')\n",
    "                 .distinct()\n",
    "                 .rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "print(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for country_code in ['US']:\n",
    "    print(\"Country code:\", country_code)\n",
    "    \n",
    "    operators = (\n",
    "        all_opr[sf.col('COUNTRY_CODE') == country_code]\n",
    "        .repartition('id')\n",
    "        .select('id', 'name')\n",
    "        .sort('id', ascending=True)\n",
    "        .rdd\n",
    "        .zipWithIndex()\n",
    "        .map(lambda x: (x[1], x[0][0], x[0][1]))\n",
    "        .toDF(schema)\n",
    "    )\n",
    "\n",
    "    \n",
    "    nvec = NameVectorizer(n_gram=2, min_df=2, vocab_size=1500).fit(operators)\n",
    "    \n",
    "    sparse_names = (nvec\n",
    "                    .transform(operators)\n",
    "                    .select(['dummy_id', 'encoded_vector']))\n",
    "\n",
    "    match_pairs = (\n",
    "        sparse_names.select('dummy_id', 'encoded_vector')\n",
    "        .withColumn('explode',\n",
    "                    sf.explode(udf_zip_sparse(sf.col('encoded_vector'))))\n",
    "        .withColumn('j_ngram', sf.col('explode').getItem('j_ngram'))\n",
    "        .withColumn('value', sf.col('explode').getItem('value'))\n",
    "        .select('dummy_id', 'j_ngram', 'value')\n",
    "    )\n",
    "\n",
    "    df = match_pairs.toPandas()\n",
    "    df.dummy_id = df.dummy_id.astype(np.int32)\n",
    "    df.j_ngram = df.j_ngram.astype(np.int32)\n",
    "    df.value = df.value.astype(np.float64)\n",
    "\n",
    "    n_matrix = csr_matrix(\n",
    "        (df.value.values, (df.dummy_id.values, df.j_ngram.values)),\n",
    "        shape=(df.dummy_id.max() + 1, df.j_ngram.max() + 1),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    del df\n",
    "\n",
    "    m_T_bcast = sc.broadcast(n_matrix.transpose())\n",
    "\n",
    "    n_chunks = max(1, math.floor(n_matrix.shape[0] / 1000))\n",
    "    chunk_size = math.ceil(n_matrix.shape[0] / n_chunks)\n",
    "    n_chunks = math.ceil(n_matrix.shape[0] / chunk_size)\n",
    "    m_chunks = [\n",
    "        (n_matrix[\n",
    "         (i * chunk_size): min((i + 1) * chunk_size, n_matrix.shape[0])],\n",
    "         i * chunk_size) for i in range(n_chunks)]\n",
    "    m_distr = sc.parallelize(m_chunks, numSlices=n_chunks)\n",
    "\n",
    "    similarity = m_distr.flatMap(\n",
    "        lambda x: chunk_dot_limit(x[0], m_T_bcast.value,\n",
    "                                  ntop=ntop,\n",
    "                                  lower_bound=threshold,\n",
    "                                  start_row=x[1])\n",
    "    )\n",
    "    m_T_bcast.unpersist()\n",
    "\n",
    "    similarity = similarity.toDF(sim_schema)\n",
    "    \n",
    "    # group similarities with column i being the group id\n",
    "    grouping_window = (Window\n",
    "                       .partitionBy('j')\n",
    "                       .orderBy(sf.asc('i')))\n",
    "\n",
    "    grp_sim = (\n",
    "        similarity\n",
    "        .withColumn(\"rn\", sf.row_number().over(grouping_window))\n",
    "        .filter(sf.col(\"rn\") == 1)\n",
    "        .drop('rn')\n",
    "    )\n",
    "\n",
    "    grp_similarity = grp_sim.join(\n",
    "        grp_sim.select('j').subtract(grp_sim.select('i')),\n",
    "        on='j', how='inner'\n",
    "    )\n",
    "\n",
    "    matches = (\n",
    "        grp_similarity\n",
    "        .join(operators, similarity['i'] == operators['dummy_id'],\n",
    "              how='left').drop('dummy_id')\n",
    "        .selectExpr('i', 'j', 'id as SOURCE_ID', 'SIMILARITY', 'name as SOURCE_NAME')\n",
    "        .join(operators, similarity['j'] == operators['dummy_id'],\n",
    "              how='left').drop('dummy_id')\n",
    "        .withColumn('matched_string', sf.lit(\n",
    "            'NAME_CLEANSED CITY_CLEANSED STREET_CLEANSED ZIP_CODE_CLEANSED'\n",
    "        ))\n",
    "        .withColumn('COUNTRY_CODE', sf.lit(country_code))\n",
    "        .selectExpr('matched_string', 'COUNTRY_CODE', 'SOURCE_ID', 'id as TARGET_ID',\n",
    "                    'SIMILARITY', 'SOURCE_NAME', 'name as TARGET_NAME')\n",
    "    )\n",
    "    \n",
    "#     print(matches.count())\n",
    "\n",
    "    of = os.path.join(output_file, \"country_code={}\".format(country_code))\n",
    "    mode = 'overwrite'\n",
    "    print(\"Writing to:\", of)\n",
    "    print(\"Mode:\", mode)\n",
    "\n",
    "    (matches\n",
    "     .write\n",
    "     .mode(mode)\n",
    "     .save(of))\n",
    "#      .parquet(output_file, mode='append'))\n",
    "\n",
    "    print(\"Done, country code:\", country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
