{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from glob import glob\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash ../compile_library.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_file = glob(os.path.join('..', 'dist', '*.egg'))[0]\n",
    "egg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"NameMatching_Notebook\")\n",
    "#              .config('spark.dynamicAllocation.enabled', False)\n",
    "         .config('spark.executorEnv.PYTHON_EGG_CACHE', '/tmp')\n",
    "#              .config('spark.executor.instances', 4)\n",
    "#              .config('spark.executor.cores', 13)\n",
    "#              .config('spark.executor.memory', '14g')\n",
    "         .config('spark.driver.memory', '7g')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "sc.addPyFile(egg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparse_dot_topn.sparse_dot_topn as ct # this is the cython code module\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dot_limit(A, B, ntop,\n",
    "                    threshold=0, start_row=0, upper_triangular=False):\n",
    "    B = B.tocsr()\n",
    "\n",
    "    M = A.shape[0]\n",
    "    N = B.shape[1]\n",
    "\n",
    "    idx_dtype = np.int32\n",
    "\n",
    "    if upper_triangular:\n",
    "        # massive memory reduction\n",
    "        # max number of possible non-zero element\n",
    "        nnz_max = min(int(M * (2 * (N - start_row) - M - 1) / 2), M * ntop)\n",
    "    else:\n",
    "        nnz_max = M * ntop\n",
    "\n",
    "    # arrays will be returned by reference\n",
    "    rows = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    cols = np.empty(nnz_max, dtype=idx_dtype)\n",
    "    data = np.empty(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    # C++ wrapped with Cython implementation\n",
    "    # number of found non-zero entries in the upper triangular matrix\n",
    "    # I'll use this value to slice the returning numpy array\n",
    "    nnz = ct.sparse_dot_topn(\n",
    "        M, N,\n",
    "        np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        threshold,\n",
    "        rows, cols, data, start_row, int(upper_triangular))\n",
    "\n",
    "    return ((int(i), int(j), float(v)) for i, j, v in\n",
    "            zip(rows[:nnz], cols[:nnz], data[:nnz]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameVectorizer(object):\n",
    "\n",
    "    def __init__(self, n_gram, min_df, vocab_size):\n",
    "        self.n_gram = n_gram\n",
    "        self.min_df = min_df\n",
    "        self.vocab_size = vocab_size\n",
    "        self.__create_pipeline()\n",
    "\n",
    "    def __create_pipeline(self):\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"name\",\n",
    "                                        outputCol=\"tokens\",\n",
    "                                        pattern=\"\")\n",
    "        ngram_creator = NGram(inputCol=\"tokens\",\n",
    "                              outputCol=\"n_grams\",\n",
    "                              n=self.n_gram)\n",
    "        tf_counter = CountVectorizer(inputCol='n_grams',\n",
    "                                     outputCol='term_frequency',\n",
    "                                     minTF=1.0,\n",
    "                                     minDF=self.min_df,\n",
    "                                     vocabSize=self.vocab_size,\n",
    "                                     binary=False)\n",
    "        idf_counter = IDF(inputCol=\"term_frequency\",\n",
    "                          outputCol=\"tfidf_vector\")\n",
    "        l2_normalizer = Normalizer(inputCol=\"tfidf_vector\",\n",
    "                                   outputCol=\"name_vector\",\n",
    "                                   p=2)\n",
    "\n",
    "        self.pipeline = Pipeline(\n",
    "            stages=[regexTokenizer,\n",
    "                    ngram_creator,\n",
    "                    tf_counter,\n",
    "                    idf_counter,\n",
    "                    l2_normalizer]\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Fit transformers and apply all estimators.\n",
    "        \"\"\"\n",
    "        return self.pipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "def unpack_vector(sparse):\n",
    "    \"\"\"Combine indices and values into a tuples.\n",
    "\n",
    "    For each value below 0.1 in the sparse vector we create a tuple and\n",
    "    then add these tuples into a single list. The tuple contains the\n",
    "    index and the value.\n",
    "    \"\"\"\n",
    "    return ((int(index), float(value)) for index, value in\n",
    "            zip(sparse.indices, sparse.values) if value > 0.05)\n",
    "\n",
    "\n",
    "schema = StructType([StructField(\"dummy_id\", LongType(), False),\n",
    "                     StructField(\"id\", StringType(), False),\n",
    "                     StructField(\"name\", StringType(), False)])\n",
    "\n",
    "ngram_schema = ArrayType(StructType([\n",
    "    StructField(\"ngram_index\", IntegerType(), False),\n",
    "    StructField(\"value\", FloatType(), False)\n",
    "]))\n",
    "\n",
    "similarity_schema = StructType([\n",
    "    StructField(\"i\", IntegerType(), False),\n",
    "    StructField(\"j\", IntegerType(), False),\n",
    "    StructField(\"SIMILARITY\", FloatType(), False)\n",
    "])\n",
    "\n",
    "udf_unpack_vector = sf.udf(unpack_vector, ngram_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS.parquet'\n",
    "output_file = 'adl://ulohubdldevne.azuredatalakestore.net/data/parquet/OPERATORS_MATCHED.parquet'\n",
    "\n",
    "input_file = '../../data/operators.parquet'\n",
    "output_file = '../../data/operators_matched.parquet'\n",
    "\n",
    "save_output = False\n",
    "mode = 'overwrite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop = 10\n",
    "threshold = 0.8\n",
    "fraction = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_chars = \"\\\\\\\\!#%&()*+-/:;<=>?@\\\\^|~\\u00A8\\u00A9\\u00AA\\u00AC\\u00AD\\u00AF\\u00B0\\u00B1\\u00B2\\u00B3\\u00B6\\u00B8\\u00B9\\u00BA\\u00BB\\u00BC\\u00BD\\u00BE\\u2013\\u2014\\u2022\\u2026\\u20AC\\u2121\\u2122\\u2196\\u2197\\u247F\\u250A\\u2543\\u2605\\u2606\\u3001\\u3002\\u300C\\u300D\\u300E\\u300F\\u3010\\u3011\\uFE36\\uFF01\\uFF06\\uFF08\\uFF09\\uFF1A\\uFF1B\\uFF1F{}\\u00AE\\u00F7\\u02F1\\u02F3\\u02F5\\u02F6\\u02F9\\u02FB\\u02FC\\u02FD\\u1BFC\\u1BFD\\u2260\\u2264\\u2DE2\\u2DF2\\uEC66\\uEC7C\\uEC7E\\uED2B\\uED34\\uED3A\\uEDAB\\uEDFC\\uEE3B\\uEEA3\\uEF61\\uEFA2\\uEFB0\\uEFB5\\uEFEA\\uEFED\\uFDAB\\uFFB7\\u007F\\u24D2\\u2560\\u2623\\u263A\\u2661\\u2665\\u266A\\u2764\\uE2B1\\uFF0D\"\n",
    "regex = \"[{}]\".format(drop_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Driver Memory: \", sc._conf.get('spark.driver.memory'))\n",
    "\n",
    "w = Window.partitionBy('COUNTRY_CODE').orderBy(sf.asc('id'))\n",
    "\n",
    "all_operators = (\n",
    "    spark\n",
    "    .read.parquet(input_file)\n",
    "    .sample(False, fraction)\n",
    "    .na.drop(subset=['NAME_CLEANSED'])\n",
    "    # create unique ID\n",
    "    .withColumn('id', sf.concat_ws('~',\n",
    "                                   sf.col('COUNTRY_CODE'),\n",
    "                                   sf.col('SOURCE'),\n",
    "                                   sf.col('REF_OPERATOR_ID')))\n",
    "    .fillna('')\n",
    "    # create string columns to matched\n",
    "    .withColumn('name',\n",
    "                sf.concat_ws(' ',\n",
    "                             sf.col('NAME_CLEANSED'),\n",
    "                             sf.col('CITY_CLEANSED'),\n",
    "                             sf.col('STREET_CLEANSED'),\n",
    "                             sf.col('ZIP_CODE_CLEANSED')))\n",
    "    .withColumn('name', sf.regexp_replace('name', regex, ''))\n",
    "    .withColumn('name', sf.trim(sf.regexp_replace('name', '\\s+', ' ')))\n",
    "    .withColumn('name_index', sf.row_number().over(w) - 1)\n",
    "    .select('name_index', 'id', 'name', 'COUNTRY_CODE')\n",
    ")\n",
    "all_operators.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_opr_count = all_operators.groupby('COUNTRY_CODE','name').count().sort('count', ascending=False)\n",
    "all_opr_count.show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opr_count = all_operators.groupby('COUNTRY_CODE').count()\n",
    "opr_count.sort('count', ascending=True).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = (opr_count[opr_count['count'] > 100]\n",
    "                 .select('COUNTRY_CODE')\n",
    "                 .distinct()\n",
    "                 .rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "print(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for country_code in country_codes:\n",
    "    operators = (\n",
    "        all_operators[sf.col('COUNTRY_CODE') == country_code]\n",
    "        .filter(sf.col('COUNTRY_CODE') == country_code)\n",
    "        .drop('COUNTRY_CODE')\n",
    "        .repartition('id')\n",
    "        .sort('id', ascending=True)\n",
    "    )\n",
    "\n",
    "    sparse_names = (\n",
    "        NameVectorizer(n_gram=2, min_df=2, vocab_size=1500)\n",
    "        .fit_transform(operators)\n",
    "        .select(['name_index', 'name_vector'])\n",
    "    )\n",
    "\n",
    "    match_pairs = (\n",
    "        sparse_names\n",
    "        .withColumn('explode', sf.explode(udf_unpack_vector(sf.col('name_vector'))))\n",
    "        .withColumn('ngram_index', sf.col('explode').getItem('ngram_index'))\n",
    "        .withColumn('value', sf.col('explode').getItem('value'))\n",
    "        .select('name_index', 'ngram_index', 'value')\n",
    "    )\n",
    "\n",
    "    df = match_pairs.toPandas()\n",
    "    df.name_index = df.name_index.astype(np.int32)\n",
    "    df.ngram_index = df.ngram_index.astype(np.int32)\n",
    "    df.value = df.value.astype(np.float64)\n",
    "\n",
    "    csr_names_vs_ngrams = csr_matrix(\n",
    "        (df.value.values, (df.name_index.values, df.ngram_index.values)),\n",
    "        shape=(df.name_index.max() + 1, df.ngram_index.max() + 1),\n",
    "        dtype=np.float64)\n",
    "    del df\n",
    "\n",
    "    csr_rdd_transpose = spark.sparkContext.broadcast(csr_names_vs_ngrams.transpose())\n",
    "\n",
    "    n_chunks = max(1, math.floor(csr_names_vs_ngrams.shape[0] / 1000))\n",
    "    chunk_size = math.ceil(csr_names_vs_ngrams.shape[0] / n_chunks)\n",
    "    n_chunks = math.ceil(csr_names_vs_ngrams.shape[0] / chunk_size)\n",
    "    chunks = [(csr_names_vs_ngrams[\n",
    "               (i * chunk_size): min((i + 1) * chunk_size, csr_names_vs_ngrams.shape[0])], i * chunk_size)\n",
    "              for i in range(n_chunks)]\n",
    "\n",
    "    chunks_rdd = spark.sparkContext.parallelize(chunks, numSlices=len(chunks))\n",
    "\n",
    "    del csr_names_vs_ngrams\n",
    "\n",
    "    similarity = chunks_rdd.flatMap(\n",
    "        lambda x: chunk_dot_limit(x[0], csr_rdd_transpose.value,\n",
    "                                  ntop=ntop,\n",
    "                                  threshold=threshold,\n",
    "                                  start_row=x[1],\n",
    "                                  upper_triangular=True)\n",
    "    )\n",
    "\n",
    "    similarity = similarity.toDF(similarity_schema)\n",
    "\n",
    "    grouping_window = (\n",
    "        Window\n",
    "        .partitionBy('j')\n",
    "        .orderBy(sf.asc('i')))\n",
    "\n",
    "    # keep only the first entry sorted alphabetically\n",
    "    grp_sim = (\n",
    "        similarity\n",
    "        .withColumn(\"rn\", sf.row_number().over(grouping_window))\n",
    "        .filter(sf.col(\"rn\") == 1)\n",
    "        .drop('rn')\n",
    "    )\n",
    "\n",
    "    # remove group ID from column j\n",
    "    grouped_similarity =  grp_sim.join(\n",
    "        grp_sim.select('j').subtract(grp_sim.select('i')),\n",
    "        on='j', how='inner'\n",
    "    )\n",
    "\n",
    "    matches = (\n",
    "        grouped_similarity\n",
    "        .join(operators, grouped_similarity['i'] == operators['name_index'],\n",
    "              how='left').drop('name_index')\n",
    "        .selectExpr('i', 'j', 'id as SOURCE_ID',\n",
    "                    'SIMILARITY', 'name as SOURCE_NAME')\n",
    "        .join(operators, grouped_similarity['j'] == operators['name_index'],\n",
    "              how='left').drop('name_index')\n",
    "        .withColumn('COUNTRY_CODE', sf.lit(country_code))\n",
    "        .selectExpr('COUNTRY_CODE', 'SOURCE_ID', 'id as TARGET_ID',\n",
    "                    'SIMILARITY', 'SOURCE_NAME', 'name as TARGET_NAME')\n",
    "    )\n",
    "\n",
    "    if save_output:\n",
    "        (matches\n",
    "         .coalesce(20)\n",
    "         .write\n",
    "         .partitionBy('country_code')\n",
    "         .parquet(fn, mode=mode))\n",
    "    else:\n",
    "        matches.persist()\n",
    "        n_matches = matches.count()\n",
    "\n",
    "        print('\\n\\nNr. Similarities:\\t', n_matches)\n",
    "        print('Threshold:\\t', threshold)\n",
    "        print('NTop:\\t', ntop)\n",
    "        (matches\n",
    "         .select('SOURCE_ID', 'TARGET_ID',\n",
    "                 'SIMILARITY', 'SOURCE_NAME', 'TARGET_NAME')\n",
    "         .sort('SIMILARITY', ascending=True)\n",
    "         .show(50, truncate=False))\n",
    "\n",
    "        (matches\n",
    "         .groupBy(['SOURCE_ID', 'SOURCE_NAME'])\n",
    "         .count()\n",
    "         .sort('count', ascending=False).show(50, truncate=False))\n",
    "\n",
    "        matches.describe('SIMILARITY').show()\n",
    "    print(\"Done, country code:\", country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
