pipelines:
  default:
    - step:
        name: Run tests
        image: timvancann/spark-sbt-ci:2.11-2.1.0
        script:
          - sbt clean coverage test coverageReport

  branches:
    master:
      - step:
          name: Run tests
          image: timvancann/spark-sbt-ci:2.11-2.1.0
          script:
            - sbt clean coverage test coverageReport
      - step:
          name: Build assembly artifact
          image: timvancann/spark-sbt-ci:2.11-2.1.0
          script:
            - sbt -DsparkDependencyType=provided assembly
          artifacts:
            - target/scala-2.11/*.jar

      - step:
          name: Deploy to azure datalake
          image: microsoft/azure-cli
          script:
            - az login --service-principal -u ${AZ_USER_ID} -p ${AZ_PASSWORD} --tenant ${AZ_TENANT_ID}
            - fn="$(ls target/scala-2.11/*.jar | cut -d'/' -f3)"
            - az storage blob upload -f target/scala-2.11/${fn} -c prod -n deployment/spark-jobs/${fn} --account-key ${AZ_STORAGE_KEY} --account-name ${AZ_STORAGE_ACCOUNT}
      - step:
          name: Deploy to databricks fs
          image: timvancann/databricks-cli
          deployment: production
          script:
            - touch ~/.databrickscfg
            - echo "[DEFAULT]" >> ~/.databrickscfg
            - echo "host = ${DB_HOST}" >> ~/.databrickscfg
            - echo "token = ${DB_TOKEN}" >> ~/.databrickscfg
            - cat ~/.databrickscfg

            - fn="$(ls target/scala-2.11/*.jar | cut -d'/' -f3)"
            - databricks fs cp --overwrite target/scala-2.11/${fn} dbfs:/libraries/ohub/${fn}
